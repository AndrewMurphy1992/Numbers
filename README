WARNING: I AM NOT RESPONSIBLE FOR YOUR LOST DATA, OR MISUSE OF AI. KEEP BACKUPS OF ANY FILES YOU USE THIS '3/4-MIRROR-CIPHER' ON. THIS IS A PRE-BETA PROGRAM AND ANY DATA YOU CODE THIS WAY IS VERY LIKELY TO BE CORRUPTED AND LOST FOREVER, OPTIMIZATIONS HAVE NOT BEEN MADE, AND IT IS NOT YET A DEPENDABLE PRODUCT. YOU HAVE BEEN WARNED.

Ok so in this portion of the pre-codec, we have a very specific problem that we would like to solve using a particular technique. Whereas in the general pre-codec that everything will go into later there is a 'latice' technique for isolating all but 1 number from the trailing positions in a 'modified_character', and the trained decoder will have specific knowledge on small words or words that may be numbers, the primary technique in this 'pre-codec' will be providing 'categorization_clues' based on manipulating large (int) and their related (str). In the main program you will notice that if you provide an AI with the numbers in order as a kind of key along with the word key and possibly the huffman code header, it will decode the ambiguous pre-coded document perfectly. Unfortunately, in documents that consist mostly of numbers this does not improve compression, but reduces it by half (pretty funny). So, in order to solve this we will apply a similar principle to the main program -- the numbers will be narrowed down using a code which provides the AI with a sort of "game". The pre-coder will provide the AI with a variety of boolean flags with associated conditionals, generated by a regular program. It will then write them into a 'number_key' which is packaged with the rest of the pre-coded document. The AI will have access to an API which contains the definitions of the flags and associated conditionals. You can think of this in terms of assembly code, with the AI acting as a type of softened-hardware, or 'Virtualized Transistor-Like Environment', if you will. This will be capable of helping reduce the size of files containing mostly numbers when used in conjunction with conventional compression, and can then be incorporated into the main program. 

Foresight: Likely, compression will be limited to 'blocks' at a time, of a certain size, because of hardware limitations and also the current limitations of AI itself. This is also an area that will probably require signifigant debugging before it could be considered a mission-critical way to compress and retrieve numbers. Still, it should be easily possible to reliably decompress numerical data using machine inference, given the correct process of elimination. It is finding a process that works 100% of the time that we are searching for here. 
